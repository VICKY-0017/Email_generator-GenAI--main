{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While no country has yet successfully landed a mission on Mars with a low budget, there are a few contenders that have made notable attempts. Here are a few examples:\n",
      "\n",
      "1. **India's Mangalyaan (Mars Orbiter Mission)**: Launched in 2013, Mangalyaan was a low-cost mission that successfully entered Mars' orbit in 2014. The mission was developed by the Indian Space Research Organisation (ISRO) with a budget of approximately $74 million, which is relatively low compared to other Mars missions.\n",
      "2. **China's Yinghuo-1**: Launched in 2011, Yinghuo-1 was a Chinese Mars orbiter that was designed to study the Martian atmosphere and geology. Although the mission failed to enter Mars' orbit due to a launch vehicle malfunction, it was notable for its low budget of approximately $100 million.\n",
      "3. **United Arab Emirates' Hope Probe**: Launched in 2020, the Hope Probe is a Mars orbiter developed by the Mohammed bin Rashid Space Centre (MBRSC) in the United Arab Emirates. The mission was designed to study the Martian atmosphere and climate, and it successfully entered Mars' orbit in 2021. The budget for the mission was approximately $200 million.\n",
      "\n",
      "While these missions have demonstrated that it's possible to explore Mars with a relatively low budget, it's worth noting that the costs of space exploration can be unpredictable and often exceed initial estimates.\n",
      "\n",
      "As for landing on Mars, that's a much more challenging and expensive endeavor. To date, only NASA's Curiosity Rover (2012) and Perseverance Rover (2020) have successfully landed on Mars, with budgets of approximately $2.5 billion and $2.4 billion, respectively.\n",
      "\n",
      "However, there are ongoing efforts to develop low-cost Mars landers, such as NASA's Mars Sample Return mission, which aims to retrieve samples from Mars and return them to Earth. The mission is currently in development, with a planned launch in the late 2020s and a budget of approximately $2.5 billion.\n",
      "\n",
      "Private companies like SpaceX and Blue Origin are also working on Mars missions, with SpaceX's Starship program aiming to establish a permanent human presence on Mars. While these missions are not necessarily \"low-budget,\" they may help drive down the costs of space exploration and make it more accessible to a wider range of organizations and countries.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"app/resources/.env\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    groq_api_key = \"groqclod_API_key\"\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"The first country to reach mars with low budget\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Llama 3.1 | Model Cards and Prompt formats\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentDocumentationTrust & SafetyCommunityTry 405B on Meta AIDownload models\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DocumentationOverviewModels Getting the Models Running Llama How-To Guides Integration Guides Community Support CommunityCommunity StoriesOpen Innovation AI Research CommunityLlama Impact GrantsResourcesAI at Meta BlogMeta NewsroomFAQPrivacy PolicyTermsCookiesTrust & SafetyOverviewResponsible Use GuideDocumentationOverviewModels Getting the Models Running Llama How-To Guides Integration Guides Community Support CommunityCommunity StoriesOpen Innovation AI Research CommunityLlama Impact GrantsResourcesAI at Meta BlogMeta NewsroomFAQPrivacy PolicyTermsCookiesTrust & SafetyOverviewResponsible Use GuideDocumentationOverviewModels Getting the Models Running Llama How-To Guides Integration Guides Community Support CommunityCommunity StoriesOpen Innovation AI Research CommunityLlama Impact GrantsResourcesAI at Meta BlogMeta NewsroomFAQPrivacy PolicyTermsCookiesTrust & SafetyOverviewResponsible Use GuideDocumentationOverviewModels Getting the Models Running Llama How-To Guides Integration Guides Community Support CommunityCommunity StoriesOpen Innovation AI Research CommunityLlama Impact GrantsResourcesAI at Meta BlogMeta NewsroomFAQPrivacy PolicyTermsCookiesTrust & SafetyOverviewResponsible Use GuideTable Of ContentsOverviewModelsLlama 3.1Llama Guard 3Prompt GuardLlama 3Llama Guard 2Other modelsGetting the ModelsMetaHugging FaceKaggle405B PartnersRunning LlamaLinuxWindowsMacCloudHow-To GuidesFine-tuningQuantizationPromptingValidationResponsible UseIntegration GuidesCode LlamaLangChainLlamalndexCommunity SupportResourcesOverviewModelsLlama 3.1Llama Guard 3Prompt GuardLlama 3Llama Guard 2Other modelsGetting the ModelsMetaHugging FaceKaggle405B PartnersRunning LlamaLinuxWindowsMacCloudHow-To GuidesFine-tuningQuantizationPromptingValidationResponsible UseIntegration GuidesCode LlamaLangChainLlamalndexCommunity SupportResourcesModel Cards & Prompt formatsLlama 3.1Model CardFor comprehensive technical information about the Llama 3.1 collection of large-language models, please see the official model card, located on GitHub.Prompt FormatThis section describes the prompt format for Llama 3.1 with an emphasis on new features. Please leverage this guidance in order to take full advantage of Llama 3.1. Note that although prompts designed for Llama 3 should work unchanged in Llama 3.1, we recommend that you update your prompts to the new format to obtain the best results.Tokens Description<|begin_of_text|>Specifies the start of the prompt<|end_of_text|>Model will cease to generate more tokens. This token is generated only by the base models.<|finetune_right_pad_id|>This token is used for padding text sequences to the same length in a batch. <|start_header_id|><|end_header_id|>These tokens enclose the role for a particular message. The possible roles are: [system, user, assistant and ipython]<|eom_id|> End of message. A message represents a possible stopping point for execution where the model can inform the executor that a tool call needs to be made. This is used for multi-step interactions between the model and any available tools. This token is emitted by the model when the Environment: ipython instruction is used in the system prompt, or if the model calls for a built-in tool.<|eot_id|> End of turn. Represents when the model has determined that it has finished interacting with the user message that initiated its response. This is used in two scenarios: at the end of a direct interaction between the model and the user at the end of multiple interactions between the model and any available toolsThis token signals to the executor that the model has finished generating a response.<|python_tag|>Is a special tag used in the model’s response to signify a tool call. Supported RolesThere are 4 different roles that are supported by Llama 3.1system: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that helps the model respond effectively. user: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model. ipython: A new role introduced in Llama 3.1. Semantically, this role means \"tool\". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.assistant: Represents the response generated by the AI model based on the context provided in the ‘system’, ‘ipython’ and ‘user’ prompts.[system, assistant, user, ipython]Llama 3.1This is the simplest way to prompt llama-based models, starting with text and allowing the model to generate new content based on the user-provided input.<|begin_of_text|>{{ user_message}}Llama 3.1 InstructNote: We recommend using Llama 70B-instruct or Llama 405B-instruct for applications that combine conversation and tool calling. Llama 8B-Instruct can not reliably maintain a conversation alongside tool calling definitions. It can be used for zero-shot tool calling, but tool instructions should be removed for regular conversations between the model and the user.The instruct models provide different customization options for prompting, from simple interactions between the user and assistant, to complex tool-calling scenarios. Below we provide a sample of prompts separated into four categories: user and assistant conversation, built-in tools in Python format, built in custom tools in JSON format, and complete custom tool formatting, with a focus on tool calling. The available options for tool calling require special attention as they can be implemented in different ways. The models are trained to identify prompts that can be answered with three different tools - Brave Web Search, Wolfram Alpha Search and Code Interpreter - and generate the appropriate Python function calls to get the answer (more details in the Built-In Tool Calling section). Additionally, custom formats can be defined in the prompts, both system and user. The model was trained to recognize novel tool definitions  in the prompt, generate tool calls based on those definitions, and to answer only in JSON format for easier parsing of the response and execution of the appropriate tools. Defining multiple tools in the system and/or user prompt can cause the model to generate multiple tool calls as a result of the user query. Developers must iterate on the prompts required to get the desired result and parse the result accordingly.Given the diverse nature of Llama model use cases, we recommend that developers thoroughly test different prompts and formats for their needs and use the guidelines provided here as a starting point for these tests. We provide an example of such a format that works well in the context of our own Llama Agent System in the final section, Define your own format.To better illustrate the flows below, this diagram shows the flow of information described:User and assistant conversationThe format for a regular multi-turn conversation between a user and the model has not changed between Llama 3 and Llama 3.1. Here is a simple example of how it's formatted. <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Built in Python based tool callingThe three built-in tools (brave_search, wolfram_alpha, and code interpreter) can be turned on using the system prompt:Brave Search: Tool call to perform web searches. Wolfram Alpha: Tool call to perform complex mathematical calculations.Code Interpreter: Enables the model to output python code.<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the current weather in Menlo Park, California?<|eot_id|><|start_header_id|>assistant<|end_header_id|>A few things to note:Just including Environment: ipython turns on code interpreter; therefore, you don’t need to specify code interpretation on the Tools: line. The model can generate python code which is interpreted by the executor, with the result provided back to the model.The message body of the assistant response starts with a special tag <|python_tag|>As alluded to above, in such an environment, the model can generate <|eom_id|> instead of just the standard <|eot_id|> . The latter indicates the turn is finished, while the former indicates continued multi-step reasoning. That is, the model is expecting a continuation message with the output of the tool call. The built-in tool is called using python syntax, and not JSON format as for zero-shot tools. The python call that the model generates for each of the tools is of the form: # for Search\n",
      "<|python_tag|>\n",
      "brave_search.call(query=\"...\")\n",
      "<|eom_id|>\n",
      "\n",
      "# for Wolfram\n",
      "<|python_tag|>\n",
      "wolfram_alpha.call(query=\"...\")\n",
      "<|eom_id|>To create custom tool calls using the zero-shot learning capability of the model, you can provide instructions for the tool definition in the system prompt along with the format you want the model to produce for the call. A working example is provided in the llama-agentic-system GitHub repository. You can check it out here. The example below shows an end-to-end interaction of prompts between the user ⇔ model ⇔ tool. This sample system uses the wolfram_alpha in-built tool. (Please note: The model does not execute the call to the tool; you need to use our llama-agentic-system, or other similar framework, to leverage the tool-calling capabilities.) Step - 1 User Prompt & System prompts<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you help me solve this equation: x^3 - 4x^2 + 6x - 24 = 0<|eot_id|><|start_header_id|>assistant<|end_header_id|>Step - 2 Model determining which tool to call<|python_tag|>wolfram_alpha.call(query=\"solve x^3 - 4x^2 + 6x - 24 = 0\")<|eom_id|>Step - 3 Response generated by tool, that is, Wolfram Alpha.{\n",
      "  \"queryresult\": {\n",
      "    \"success\": true,\n",
      "    \"inputstring\": \"solve x^3 - 4x^2 + 6x - 24 = 0\",\n",
      "    \"pods\": [\n",
      "      {\n",
      "        \"title\": \"Input interpretation\",\n",
      "        \"subpods\": [\n",
      "          {\n",
      "            \"title\": \"\",\n",
      "            \"plaintext\": \"solve x^3 - 4 x^2 + 6 x - 24 = 0\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"title\": \"Results\",\n",
      "        \"primary\": true,\n",
      "        \"subpods\": [\n",
      "          {\n",
      "            \"title\": \"\",\n",
      "            \"plaintext\": \"x = 4\"\n",
      "          },\n",
      "          {\n",
      "            \"title\": \"\",\n",
      "            \"plaintext\": \"x = ± (i sqrt(6))\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      ...\n",
      "    ]\n",
      "  }\n",
      "}Step  - 4 Reprompt Model with tool response<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "\n",
      "Can you help me solve this equation: x^3 - 4x^2 + 6x - 24 = 0<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|python_tag|>wolfram_alpha.call(query=\"solve x^3 - 4x^2 + 6x - 24 = 0\")<|eom_id|><|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\"queryresult\": {\"success\": true, \"inputstring\": \"solve x^3 - 4x^2 + 6x - 24 = 0\", \"pods\": [{\"title\": \"Input interpretation\", \"subpods\": [{\"title\": \"\", \"plaintext\": \"solve x^3 - 4 x^2 + 6 x - 24 = 0\"}]}, {\"title\": \"Results\", \"primary\": true, \"subpods\": [{\"title\": \"\", \"plaintext\": \"x = 4\"}, {\"title\": \"\", \"plaintext\": \"x = \\u00b1 (i sqrt(6))\"}]}, ... ]}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>Step - 5 Response from Agent to UserThe solutions to the equation x^3 - 4x^2 + 6x - 24 = 0 are x = 4 and x = ±(i√6).<|eot_id|>JSON based  tool callingMeta Llama models can now output custom tool calls from a single message to allow easier tool calling. The following prompts provide an example of how custom tools can be called from the output of the model. It's important to note that the model itself does not execute the calls; it provides structured output to facilitate calling by an executor. An example executor can be found in the llama-agentic-system. The tool format is similar to the OpenAI definition and can be adjusted to better suit your needs, as noted in the following section. NotesWith custom tool calling, for the model to output the eom_id, the following instruction needs to be added to the system prompt: Environment: ipython. Else, it should output eot_id.The system prompt needs to be adjusted to inform the model how to deal with tool call outputs The tool definition is provided in the user prompt, as that is how the model was trained for the built in JSON tool calling. However, it's possible to provide the tool definition in the system prompt as well—and get similar results. Developers must test which way works best for their use case.Step - 1 User prompt with custom tool details<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "    \"name\": \"get_current_conditions\",\n",
      "    \"description\": \"Get the current weather conditions for a specific location\",\n",
      "    \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "        \"location\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
      "        },\n",
      "        \"unit\": {\n",
      "            \"type\": \"string\",\n",
      "            \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
      "            \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
      "        }\n",
      "        },\n",
      "        \"required\": [\"location\", \"unit\"]\n",
      "    }\n",
      "    }\n",
      "}\n",
      "\n",
      "Question: what is the weather like in San Fransisco?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Step - 2 Model determining which tool to callNote:As we are not using the Environment: ipython instruction in the system prompt, the model ends generation with the eot_id token, not the eom_id token{\"name\": \"get_current_conditions\", \"parameters\": {\"location\": \"San Francisco, CA\", \"unit\": \"Fahrenheit\"}}<eot_id>Step - 3 Result from calling the tool is passed back to the modelWhen the result of the tool call is sent back to the model, we use the new role, ipython, in the header.<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "    \"name\": \"get_current_conditions\",\n",
      "    \"description\": \"Get the current weather conditions for a specific location\",\n",
      "    \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "        \"location\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
      "        },\n",
      "        \"unit\": {\n",
      "            \"type\": \"string\",\n",
      "            \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
      "            \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
      "        }\n",
      "        },\n",
      "        \"required\": [\"location\", \"unit\"]\n",
      "    }\n",
      "    }\n",
      "}\n",
      "\n",
      "Question: what is the weather like in San Fransisco?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"name\": \"get_current_conditions\", \"parameters\": {\"location\": \"San Francisco, CA\", \"unit\": \"Fahrenheit\"}}<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{\"output\": \"Clouds giving way to sun Hi: 76° Tonight: Mainly clear early, then areas of low clouds forming Lo: 56°\"}<|eot_id|><|start_header_id|>assistant<|end_header_id|>Step - 4 The model generates the final response the userThe weather in Menlo Park is currently cloudy with a high of 76° and a low of 56°, with clear skies expected tonight.<eot_id>User-defined Custom tool callingThe following prompts provide an example of how to define your own format in the system prompt and make use of it while interacting with the model. This format will use a new and custom <function> tag  to enclose a JSON object with the function parameters. NotesThe model uses the  eom_id token because of the Environment: ipython instruction.The system prompt defines the custom format as well as the tools the model has available. Step - 1 System prompt with custom tool details<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "# Tool Instructions\n",
      "- Always execute python code in messages that you share.\n",
      "- When looking for real time information use relevant functions if available else fallback to brave_search\n",
      "\n",
      "\n",
      "\n",
      "You have access to the following functions:\n",
      "\n",
      "Use the function 'spotify_trending_songs' to: Get top trending songs on Spotify\n",
      "{\n",
      "  \"name\": \"spotify_trending_songs\",\n",
      "  \"description\": \"Get top trending songs on Spotify\",\n",
      "  \"parameters\": {\n",
      "    \"n\": {\n",
      "      \"param_type\": \"int\",\n",
      "      \"description\": \"Number of trending songs to get\",\n",
      "      \"required\": true\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "If a you choose to call a function ONLY reply in the following format:\n",
      "<{start_tag}={function_name}>{parameters}{end_tag}\n",
      "where\n",
      "\n",
      "start_tag => `<function`\n",
      "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
      "end_tag => `</function>`\n",
      "\n",
      "Here is an example,\n",
      "<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n",
      "\n",
      "Reminder:\n",
      "- Function calls MUST follow the specified format\n",
      "- Required parameters MUST be specified\n",
      "- Only call one function at a time\n",
      "- Put the entire function call reply on one line\n",
      "- Always add your sources when using search results to answer the user query\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you check the top 5 trending songs on spotify?<|eot_id|><|start_header_id|>assistant<|end_header_id|>                      Step - 2 Model determining which tool to callNow the model responds in accordance with the format defined in the system prompt, with the function name in the start tag, and the parameters, as a JSON object, in between the start and end tags. Note: As we are using the Environment: ipython instruction in the system prompt, the model ends generation with the eom_id token.<function=spotify_trending_songs>{\"n\": \"5\"}</function><eom_id>Step - 3 Result from calling the tool is passed back to the modelWhen the result of the tool call is sent back to the model, we use the new role, ipython, in the header.<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "# Tool Instructions\n",
      "- Always execute python code in messages that you share.\n",
      "- When looking for real time information use relevant functions if available else fallback to brave_search\n",
      "\n",
      "\n",
      "\n",
      "You have access to the following functions:\n",
      "\n",
      "Use the function 'spotify_trending_songs' to: Get top trending songs on Spotify\n",
      "{\"name\": \"spotify_trending_songs\", \"description\": \"Get top trending songs on Spotify\", \"parameters\": {\"n\": {\"param_type\": \"int\", \"description\": \"Number of trending songs to get\", \"required\": true}}}\n",
      "\n",
      "\n",
      "If a you choose to call a function ONLY reply in the following format:\n",
      "<{start_tag}={function_name}>{parameters}{end_tag}\n",
      "where\n",
      "\n",
      "start_tag => `<function`\n",
      "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
      "end_tag => `</function>`\n",
      "\n",
      "Here is an example,\n",
      "<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n",
      "\n",
      "Reminder:\n",
      "- Function calls MUST follow the specified format\n",
      "- Required parameters MUST be specified\n",
      "- Only call one function at a time\n",
      "- Put the entire function call reply on one line\n",
      "- Always add your sources when using search results to answer the user query\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you check the top 5 trending songs on spotify?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<function=spotify_trending_songs>{\"n\": \"5\"}</function><|eom_id|><|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "[\"1. BIRDS OF A FEATHER by Billie Eilish\", \"2. Espresso by Sabrina Carpenter\", \"3. Please Please Please by Sabrina Carpenter\", \"4. Not Like Us by Kendrick Lamar\", \"5. Gata Only by FloyyMenor, Cris Mj\"]<|eot_id|><|start_header_id|>assistant<|end_header_id|>                      Step - 4 The model generates the final response the userThe top 5 trending songs on Spotify are:\n",
      "\n",
      "1. BIRDS OF A FEATHER by Billie Eilish\n",
      "2. Espresso by Sabrina Carpenter\n",
      "3. Please Please Please by Sabrina Carpenter\n",
      "4. Not Like Us by Kendrick Lamar\n",
      "5. Gata Only by FloyyMenor, Cris Mj<|eot_id|>Was this page helpful?YesNoOn this pageLlama 3.1Model CardPrompt FormatSupported RolesLlama 3.1Llama 3.1 InstructUser and assistant conversationBuilt in Python based tool callingJSON based  tool callingUser-defined Custom tool calling\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/\")\n",
    "page_data = loader.load().pop().page_content\n",
    "\n",
    "print(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"role\": \"Software Engineer\",\n",
      "    \"experience\": \"3-5 years\",\n",
      "    \"skills\": \"Python, Java, C++, AWS, Azure\",\n",
      "    \"description\": \"Design, develop, and test software applications\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"Data Scientist\",\n",
      "    \"experience\": \"5-7 years\",\n",
      "    \"skills\": \"Python, R, SQL, Machine Learning, Data Visualization\",\n",
      "    \"description\": \"Analyze and interpret complex data to inform business decisions\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"DevOps Engineer\",\n",
      "    \"experience\": \"3-5 years\",\n",
      "    \"skills\": \"AWS, Azure, Docker, Kubernetes, Jenkins\",\n",
      "    \"description\": \"Ensure the smooth operation of software systems and infrastructure\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_extract = PromptTemplate.from_template(\n",
    "    \n",
    "    \"\"\"\n",
    "        ### SCRAPED TEXT FROM WEBSITE:\n",
    "        {page_data}\n",
    "        ### INSTRUCTION:\n",
    "        The scraped text is from the career's page of a website.\n",
    "        Your job is to extract the job postings and return them in JSON format containing the \n",
    "        following keys: `role`, `experience`, `skills` and `description`.\n",
    "        Only return the valid JSON.\n",
    "        ### VALID JSON (NO PREAMBLE):    \n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "chain_extract = prompt_extract | llm\n",
    "\n",
    "res = chain_extract.invoke(input = {'page_data':page_data})\n",
    "type(res.content)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'Software Engineer',\n",
       "  'experience': '3-5 years',\n",
       "  'skills': 'Python, Java, C++, AWS, Azure',\n",
       "  'description': 'Design, develop, and test software applications'},\n",
       " {'role': 'Data Scientist',\n",
       "  'experience': '5-7 years',\n",
       "  'skills': 'Python, R, SQL, Machine Learning, Data Visualization',\n",
       "  'description': 'Analyze and interpret complex data to inform business decisions'},\n",
       " {'role': 'DevOps Engineer',\n",
       "  'experience': '3-5 years',\n",
       "  'skills': 'AWS, Azure, Docker, Kubernetes, Jenkins',\n",
       "  'description': 'Ensure the smooth operation of software systems and infrastructure'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "json_res = json_parser.parse(res.content)\n",
    "json_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Techstack</th>\n",
       "      <th>Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>React, Node.js, MongoDB</td>\n",
       "      <td>https://example.com/react-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angular,.NET, SQL Server</td>\n",
       "      <td>https://example.com/angular-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vue.js, Ruby on Rails, PostgreSQL</td>\n",
       "      <td>https://example.com/vue-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python, Django, MySQL</td>\n",
       "      <td>https://example.com/python-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Java, Spring Boot, Oracle</td>\n",
       "      <td>https://example.com/java-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Flutter, Firebase, GraphQL</td>\n",
       "      <td>https://example.com/flutter-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WordPress, PHP, MySQL</td>\n",
       "      <td>https://example.com/wordpress-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Magento, PHP, MySQL</td>\n",
       "      <td>https://example.com/magento-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>React Native, Node.js, MongoDB</td>\n",
       "      <td>https://example.com/react-native-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>iOS, Swift, Core Data</td>\n",
       "      <td>https://example.com/ios-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Android, Java, Room Persistence</td>\n",
       "      <td>https://example.com/android-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kotlin, Android, Firebase</td>\n",
       "      <td>https://example.com/kotlin-android-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Android TV, Kotlin, Android NDK</td>\n",
       "      <td>https://example.com/android-tv-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>iOS, Swift, ARKit</td>\n",
       "      <td>https://example.com/ios-ar-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cross-platform, Xamarin, Azure</td>\n",
       "      <td>https://example.com/xamarin-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Backend, Kotlin, Spring Boot</td>\n",
       "      <td>https://example.com/kotlin-backend-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Frontend, TypeScript, Angular</td>\n",
       "      <td>https://example.com/typescript-frontend-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Full-stack, JavaScript, Express.js</td>\n",
       "      <td>https://example.com/full-stack-js-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Machine Learning, Python, TensorFlow</td>\n",
       "      <td>https://example.com/ml-python-portfolio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DevOps, Jenkins, Docker</td>\n",
       "      <td>https://example.com/devops-portfolio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Techstack  \\\n",
       "0                React, Node.js, MongoDB   \n",
       "1               Angular,.NET, SQL Server   \n",
       "2      Vue.js, Ruby on Rails, PostgreSQL   \n",
       "3                  Python, Django, MySQL   \n",
       "4              Java, Spring Boot, Oracle   \n",
       "5             Flutter, Firebase, GraphQL   \n",
       "6                  WordPress, PHP, MySQL   \n",
       "7                    Magento, PHP, MySQL   \n",
       "8         React Native, Node.js, MongoDB   \n",
       "9                  iOS, Swift, Core Data   \n",
       "10       Android, Java, Room Persistence   \n",
       "11             Kotlin, Android, Firebase   \n",
       "12       Android TV, Kotlin, Android NDK   \n",
       "13                     iOS, Swift, ARKit   \n",
       "14        Cross-platform, Xamarin, Azure   \n",
       "15          Backend, Kotlin, Spring Boot   \n",
       "16         Frontend, TypeScript, Angular   \n",
       "17    Full-stack, JavaScript, Express.js   \n",
       "18  Machine Learning, Python, TensorFlow   \n",
       "19               DevOps, Jenkins, Docker   \n",
       "\n",
       "                                                Links  \n",
       "0                 https://example.com/react-portfolio  \n",
       "1               https://example.com/angular-portfolio  \n",
       "2                   https://example.com/vue-portfolio  \n",
       "3                https://example.com/python-portfolio  \n",
       "4                  https://example.com/java-portfolio  \n",
       "5               https://example.com/flutter-portfolio  \n",
       "6             https://example.com/wordpress-portfolio  \n",
       "7               https://example.com/magento-portfolio  \n",
       "8          https://example.com/react-native-portfolio  \n",
       "9                   https://example.com/ios-portfolio  \n",
       "10              https://example.com/android-portfolio  \n",
       "11       https://example.com/kotlin-android-portfolio  \n",
       "12           https://example.com/android-tv-portfolio  \n",
       "13               https://example.com/ios-ar-portfolio  \n",
       "14              https://example.com/xamarin-portfolio  \n",
       "15       https://example.com/kotlin-backend-portfolio  \n",
       "16  https://example.com/typescript-frontend-portfolio  \n",
       "17        https://example.com/full-stack-js-portfolio  \n",
       "18            https://example.com/ml-python-portfolio  \n",
       "19               https://example.com/devops-portfolio  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"my_portfolio.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient('VectorStore')\n",
    "collection = client.get_or_create_collection(name=\"portfolio\")\n",
    "\n",
    "if not collection.count():\n",
    "    for _,row in df.iterrows():\n",
    "        collection.add(documents = row[\"Techstack\"],\n",
    "                       metadatas = {\"links\":row[\"Links\"]},\n",
    "                       ids=[str(uuid.uuid4())]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'links': 'https://example.com/ml-python-portfolio'},\n",
       "  {'links': 'https://example.com/python-portfolio'}],\n",
       " [{'links': 'https://example.com/react-portfolio'},\n",
       "  {'links': 'https://example.com/react-native-portfolio'}],\n",
       " [{'links': 'https://example.com/ml-python-portfolio'},\n",
       "  {'links': 'https://example.com/python-portfolio'}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = collection.query(query_texts=[\"Experience in Python\",\"Expertise in React\",\"Expertise in Machine Learning\"],n_results=2).get('metadatas',[])\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[43mjob\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRole: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Skills: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskills\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'job' is not defined"
     ]
    }
   ],
   "source": [
    "for j in job:\n",
    "    print(f\"Role: {j['role']}, Skills: {j['skills']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Expert Software Solutions for Your Business Needs\n",
      "\n",
      "Dear [Client's Name],\n",
      "\n",
      "I hope this email finds you well. As a Business Development Executive at AtliQ, I came across your job postings for a Software Engineer, Data Scientist, and DevOps Engineer. I am excited to introduce you to our team of experts who can help you achieve your business goals through cutting-edge software solutions.\n",
      "\n",
      "At AtliQ, we specialize in AI and software consulting, empowering businesses like yours to streamline processes, enhance scalability, and reduce costs. Our team of seasoned professionals has a proven track record of delivering tailored solutions that meet the unique needs of our clients.\n",
      "\n",
      "Our capabilities align perfectly with your job requirements. We have expertise in Python, Java, C++, AWS, Azure, and other technologies that your job postings mention. Our team can design, develop, and test software applications that meet your business needs. We can also analyze and interpret complex data to inform your business decisions, and ensure the smooth operation of your software systems and infrastructure.\n",
      "\n",
      "I would like to showcase some of our relevant work:\n",
      "\n",
      "* Machine Learning and Python expertise: https://example.com/ml-python-portfolio\n",
      "* Python-based solutions: https://example.com/python-portfolio\n",
      "* React and React Native capabilities: https://example.com/react-portfolio, https://example.com/react-native-portfolio\n",
      "\n",
      "These examples demonstrate our ability to deliver high-quality software solutions that drive business success. I would be delighted to schedule a call to discuss how AtliQ can help you achieve your goals.\n",
      "\n",
      "Please let me know if you're interested in learning more about our services. I look forward to the opportunity to collaborate with you.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Mohan\n",
      "Business Development Executive\n",
      "AtliQ\n"
     ]
    }
   ],
   "source": [
    "prompt_email = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        ### JOB DESCRIPTION:\n",
    "        {job_description}\n",
    "        \n",
    "        ### INSTRUCTION:\n",
    "        You are Mohan, a business development executive at AtliQ. AtliQ is an AI & Software Consulting company dedicated to facilitating\n",
    "        the seamless integration of business processes through automated tools. \n",
    "        Over our experience, we have empowered numerous enterprises with tailored solutions, fostering scalability, \n",
    "        process optimization, cost reduction, and heightened overall efficiency. \n",
    "        Your job is to write a cold email to the client regarding the job mentioned above describing the capability of AtliQ \n",
    "        in fulfilling their needs.\n",
    "        Also add the most relevant ones from the following links to showcase Atliq's portfolio: {link_list}\n",
    "        Remember you are Mohan, BDE at AtliQ. \n",
    "        Do not provide a preamble.\n",
    "        ### EMAIL (NO PREAMBLE):\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    ")\n",
    "\n",
    "chain_email = prompt_email | llm\n",
    "res = chain_email.invoke({\"job_description\":str(job),\"link_list\":links})\n",
    "print(res.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
